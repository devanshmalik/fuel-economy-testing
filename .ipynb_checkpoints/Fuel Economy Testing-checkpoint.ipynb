{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuel Economy Testing\n",
    "Exploration and analysis of Fuel Economy data obtained from vehicle testing done by EPA (Environmental Protection Agency) at National Vehicle and Fuel Emissions laboratory in Ann Arbor, Michigan.\n",
    "\n",
    "Data for years 2008 and 2018 was chosen to explore changes in car emissions and fuel economy over a decade.\n",
    "\n",
    "Source: [EPA Fuel Economy Data](https://www.fueleconomy.gov/feg/download.shtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Assessing \n",
    "General exploration of datasets for number of samples, missing values, duplicate rows, etc. \n",
    "\n",
    "Files: `all_alpha_18.csv`, `all_alpha_08.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and load dataset\n",
    "import pandas as pd\n",
    "\n",
    "df_18 = pd.read_csv('all_alpha_18.csv')\n",
    "df_08 = pd.read_csv('all_alpha_08.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples / columns in each dataset\n",
    "print(\"2018 samples and columns: {}\".format(df_18.shape))\n",
    "print(\"2008 samples and columns: {}\".format(df_08.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns and datatypes info\n",
    "df_18.info()\n",
    "df_08.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with missing values\n",
    "missing_18 = df_18.columns[df_18.isnull().any()]\n",
    "print('Columns with missing values in 2018 data: {}'.format(missing_18))\n",
    "\n",
    "# Rows with null values in each column\n",
    "df_18.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with missing values\n",
    "missing_08 = df_08.columns[df_08.isnull().any()]\n",
    "print('Columns with missing values in 2008 data: {}'.format(missing_08))\n",
    "\n",
    "# Rows with null values in each column\n",
    "df_08.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2008 dataset has more missing values for multiple columns versus 2018 dataset which is missing values only in `Displ` and `Cyl` columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate rows in dataset\n",
    "duplicate_18 = df_18[df_18.duplicated(keep=False)]\n",
    "duplicate_08 = df_08[df_08.duplicated(keep=False)]\n",
    "\n",
    "# Uncomment to explore duplicate rows\n",
    "#print(\"2018 Duplicate Rows: \\n{}\".format(duplicate_18))\n",
    "#print(\"2008 Duplicate Rows: \\n{}\".format(duplicate_08))\n",
    "\n",
    "# Number of duplicated rows\n",
    "df_18.duplicated(keep='last').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate rows in dataset\n",
    "duplicate_18 = df_18[df_18.duplicated(keep=False)]\n",
    "duplicate_08 = df_08[df_08.duplicated(keep=False)]\n",
    "\n",
    "# Uncomment to explore duplicate rows\n",
    "#print(\"2008 Duplicate Rows: \\n{}\".format(duplicate_08))\n",
    "\n",
    "# Number of duplicated rows\n",
    "df_08.duplicated(keep='last').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018 dataset contains no duplicate values while the 2008 dataset contains 25 duplicated rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-unique values in each column\n",
    "#print(\"\\nUnique values in 2018 dataset: \\n{}\".format(df_18.nunique()))\n",
    "#print(\"\\nUnique values in 2008 dataset: \\n{}\".format(df_08.nunique()))\n",
    "\n",
    "# Unique values and counts for each in specific columns \n",
    "print(df_18['Cyl'].value_counts())\n",
    "print(df_08['Cyl'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Cleaning Columns Labels \n",
    "Drop extraneous columns and rename applicable columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View 2018 dataset\n",
    "df_18.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View 2008 dataset\n",
    "df_08.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Extraneous Columns \n",
    "\n",
    "Columns not present in both datasets or irrelevant to analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns from 2008 dataset\n",
    "df_08 = df_08.drop(columns=['Stnd', 'Underhood ID', 'FE Calc Appr', 'Unadj Cmb MPG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns from 2018 dataset\n",
    "df_18 = df_18.drop(columns=['Stnd', 'Stnd Description', 'Underhood ID', 'Comb CO2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns \n",
    "Update column names for consistency and rename column names to lowercase without spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"Sales Area\" column in 2008 to \"Cert Region\"\n",
    "df_08.rename(columns = {'Sales Area': 'Cert Region'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace spaces with underscore\n",
    "df_08.columns = df_08.columns.str.replace('\\s+', '_')\n",
    "df_18.columns = df_18.columns.str.replace('\\s+', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names to lower\n",
    "df_08.columns = df_08.columns.str.lower()\n",
    "df_18.columns = df_18.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm datasets are identical \n",
    "(df_08.columns == df_18.columns).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new datasets for next section\n",
    "df_08.to_csv('data_08.csv', index=False)\n",
    "df_18.to_csv('data_18.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter, Drop Nulls, Dedupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Certification\n",
    "Filter both datasets for cars where `cert_region` is `CA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for cert_region CA\n",
    "df_08 = df_08.query(\"cert_region == 'CA'\")\n",
    "df_18 = df_18.query(\"cert_region == 'CA'\")\n",
    "\n",
    "# Drop cert_region column\n",
    "df_08 = df_08.drop(columns=['cert_region'])\n",
    "df_18 = df_18.drop(columns=['cert_region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Nulls\n",
    "Drop any rows in both datasets that contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls\n",
    "df_08.dropna(inplace=True)\n",
    "df_18.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows with null value in 2008 - should print False\n",
    "df_08.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows with null value in 2018 - should print False\n",
    "df_18.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dedupe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of duplicates in 2008 and 2018\n",
    "print(\"Duplicates in 2008: {}\".format(df_08.duplicated().sum()))\n",
    "print(\"Duplicates in 2018: {}\".format(df_18.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates in both datasets\n",
    "df_08.drop_duplicates(inplace=True)\n",
    "df_18.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm dedupe - should both be 0 \n",
    "print(\"Duplicates in 2008: {}\".format(df_08.duplicated().sum()))\n",
    "print(\"Duplicates in 2018: {}\".format(df_18.duplicated().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new datasets for next section\n",
    "df_08.to_csv('data_08.csv', index=False)\n",
    "df_18.to_csv('data_18.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_08.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_18.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datatype Fixing\n",
    "Convert data type between datasets for consistency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix `cyl` datatype\n",
    "* 2008 - Extract int from string\n",
    "* 2018 - Convert float to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract int from string\n",
    "df_08['cyl'] = df_08['cyl'].str[1:2].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 2018 cyl to int\n",
    "df_18['cyl'] = df_18['cyl'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm datatype change - should be int \n",
    "df_18['cyl'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix `air_pollution_score`  Data Type\n",
    "* 2008: convert string to float\n",
    "** Cannot be converted directly as some rows contain multiple scores separated by `/`. Example: `6/4`. This is becuase \"If a vehicle can operate on more than one type of fuel, an estimate is provided for each fuel type.\"\n",
    "* 2018: convert int to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all hybrid entries in 08\n",
    "hb_08 = df_08[df_08['fuel'].str.contains('/')]\n",
    "hb_08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two copies of the 2008 hybrids dataframe\n",
    "df1 = hb_08.copy()  # data on first fuel type of each hybrid vehicle\n",
    "df2 = hb_08.copy()  # data on second fuel type of each hybrid vehicle\n",
    "\n",
    "# Each one should look like this\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to split by \"/\"\n",
    "split_columns = ['fuel','air_pollution_score', 'city_mpg', 'hwy_mpg', 'cmb_mpg', 'greenhouse_gas_score']\n",
    "\n",
    "# apply split function to each column of each dataframe copy\n",
    "for c in split_columns:\n",
    "    df1[c] = df1[c].apply(lambda x: x.split(\"/\")[0])\n",
    "    df2[c] = df2[c].apply(lambda x: x.split(\"/\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dataframes to add to the original dataframe\n",
    "new_rows = df1.append(df2)\n",
    "\n",
    "# now we have s`eparate rows for each fuel type of each vehicle!\n",
    "new_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original hybrid rows\n",
    "df_08.drop(hb_08.index, inplace=True)\n",
    "\n",
    "# add in our newly separated rows\n",
    "df_08 = df_08.append(new_rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat hybrid entries split for 2018 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all hybrid entries in 18\n",
    "hb_18 = df_18[df_18['fuel'].str.contains('/')]\n",
    "hb_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two copies of the 2008 hybrids dataframe\n",
    "df1 = hb_18.copy()  # data on first fuel type of each hybrid vehicle\n",
    "df2 = hb_18.copy()  # data on second fuel type of each hybrid vehicle\n",
    "\n",
    "# Each one should look like this\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to split by \"/\"\n",
    "split_columns = ['fuel', 'city_mpg', 'hwy_mpg', 'cmb_mpg']\n",
    "\n",
    "# apply split function to each column of each dataframe copy\n",
    "for c in split_columns:\n",
    "    df1[c] = df1[c].apply(lambda x: x.split(\"/\")[0])\n",
    "    df2[c] = df2[c].apply(lambda x: x.split(\"/\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dataframes to add to the original dataframe\n",
    "new_rows = df1.append(df2)\n",
    "\n",
    "# now we have separate rows for each fuel type of each vehicle!\n",
    "new_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original hybrid rows\n",
    "df_18.drop(hb_18.index, inplace=True)\n",
    "\n",
    "# add in our newly separated rows\n",
    "df_18 = df_18.append(new_rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string to float for 2008 air pollution column\n",
    "df_08['air_pollution_score'] = df_08['air_pollution_score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert int to float for 2018 air pollution column\n",
    "df_18['air_pollution_score'] = df_18['air_pollution_score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_08.to_csv('data_08.csv', index=False)\n",
    "df_18.to_csv('data_18.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix `city_mpg`, `hwy_mpg`, `cmb_mpg` datatypes\n",
    "    2008 and 2018: convert string to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mpg columns to floats\n",
    "mpg_columns = ['city_mpg', 'hwy_mpg', 'cmb_mpg']\n",
    "for c in mpg_columns:\n",
    "    df_18[c] = df_18[c].astype(float)\n",
    "    df_08[c] = df_08[c].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix `greenhouse_gas_score` datatype\n",
    "    2008: convert from float to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from float to int\n",
    "df_08['greenhouse_gas_score'] = df_08['greenhouse_gas_score'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm Data Type changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_08.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_18.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CLEAN datasets as new files!\n",
    "df_08.to_csv('clean_08.csv', index=False)\n",
    "df_18.to_csv('clean_18.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import clean datasets\n",
    "clean_08 = pd.read_csv('clean_08.csv')\n",
    "clean_18 = pd.read_csv('clean_18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename 2008 columns \n",
    "renamed_df_08 = clean_08.rename(columns = lambda x: x[:10] + \"_2008\" if x in clean_08.columns else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_df_08.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_18.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "df_combined = pd.merge(renamed_df_08, clean_18, how='inner', left_on='model_2008', right_on='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view to check merge\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('combined_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
